# AI Me - Style Transfer Evaluation

This directory contains everything needed to evaluate your fine-tuned LoRA model's ability to transfer your personal writing style using **two separate scripts** for optimal workflow.

## 🎯 What We're Evaluating

The evaluation compares four versions of email responses:
1. **Original**: Your actual written responses from real emails (2020 onwards)
2. **GPT-4**: Responses generated by GPT-4 API to the same contexts
3. **LoRA Response**: Direct responses generated by your trained LoRA model
4. **LoRA Rewrite**: GPT-4 responses rewritten using your trained LoRA model to apply your personal style

### **Hybrid Approach Benefits:**
- **GPT-4 Intelligence**: Handles content, reasoning, and knowledge
- **LoRA Style**: Applies your personal writing style, tone, and phrasing
- **Best of Both**: Leverages GPT-4's capabilities with your personal style

## 🚀 Two-Script Approach

### **Why Two Scripts?**

- **Script 1 (Colab)**: Generates dataset with GPU access for LoRA model
- **Script 2 (Local)**: Performs comprehensive analysis without GPU requirements
- **Better Workflow**: Separate concerns, easier debugging, flexible execution

## 📁 Directory Structure

```
ai_me/
├── evaluation/                          # This directory
│   ├── README.md                       # This file
│   ├── generate_evaluation_dataset.py   # Script 1: Dataset generation (Colab)
│   ├── analyze_evaluation_dataset.py    # Script 2: Analysis (Local)
│   ├── select_evaluation_emails.py     # Email selection helper
│   ├── evaluation_requirements.txt     # Complete dependencies
│   └── evaluation_data/
│       └── selected_evaluation_emails.json     # 50 selected emails (2020+)
├── data/                               # Your email data
├── fine_tuning/                        # LoRA training files
└── ...                                 # Other project files
```

## 🔧 Script 1: Dataset Generation (Colab)

**File**: `generate_evaluation_dataset.py`  
**Purpose**: Generate the evaluation dataset with all three text versions  
**Run in**: Google Colab (GPU access required)  
**Output**: `evaluation_data/evaluation_dataset.json`

### **What it does:**
1. Loads selected emails from your data
2. Generates GPT-4 responses using OpenAI API
3. Generates direct LoRA responses to partner emails
4. **Rewrites GPT-4 responses using LoRA model** (hybrid approach)
5. Saves complete dataset for analysis

### **Configuration:**
```python
OPENAI_API_KEY = "your_openai_api_key_here"
LORA_MODEL_PATH = "./fine_tuning/out/ai_me_lora_llama3p1_8b"
NUM_SAMPLES = 50  # Number of emails to evaluate
```

### **Usage in Colab:**
```python
# Upload the script and run
!python generate_evaluation_dataset.py

# Download the generated dataset
from google.colab import files
files.download('evaluation_data/evaluation_dataset.json')
```

## 🔍 Script 2: Comprehensive Analysis (Local)

**File**: `analyze_evaluation_dataset.py`  
**Purpose**: Perform detailed style analysis using advanced metrics  
**Run in**: Any environment (no GPU required)  
**Input**: `evaluation_data/evaluation_dataset.json`  
**Output**: `evaluation_results/` directory with comprehensive results

### **Advanced Metrics Calculated:**

#### **1. Authorship Verification (AUC)**
- **Purpose**: Measures how well the model distinguishes your style
- **Method**: Logistic regression classifier with ROC-AUC scoring
- **Interpretation**: Higher AUC = better style differentiation

#### **2. Burrow's Delta**
- **Purpose**: Statistical authorship attribution measure
- **Method**: Z-score based feature comparison
- **Interpretation**: Lower Delta = more similar writing style

#### **3. Cosine Similarity**
- **Purpose**: Vector-based style similarity
- **Method**: TF-IDF vectorization with cosine distance
- **Interpretation**: Higher score = more similar style

#### **4. Character N-gram Analysis**
- **Purpose**: Character-level style pattern analysis
- **Features**: 3-gram and 4-gram entropy, type-token ratio
- **Interpretation**: Captures subtle character usage patterns

#### **5. Lexical Richness Metrics**
- **Type-Token Ratio**: Vocabulary diversity
- **Hapax Legomena**: Unique word usage
- **Yule's K**: Vocabulary richness
- **Simpson's D**: Word frequency distribution
- **Brunet's W**: Vocabulary size estimation
- **Honoré's Statistic**: Rare word usage

#### **6. Syntactic Complexity**
- **Function Word Ratios**: Prepositions, conjunctions, articles
- **Sentence Structure**: Length, complexity, clause analysis
- **Grammar Patterns**: Part-of-speech distributions

#### **7. Basic Style Features**
- **Contraction Usage**: Informal language indicators
- **Punctuation Patterns**: Exclamation, question marks
- **Personal Pronouns**: I, you, we, etc.
- **Casual Expressions**: hey, cool, awesome, etc.

## 📊 Output Files

### **From Script 1 (Dataset Generation):**
- `evaluation_data/evaluation_dataset.json` - Complete dataset with all four versions

### **From Script 2 (Analysis):**
- `evaluation_results/analysis_results.json` - Detailed analysis data
- `evaluation_results/comprehensive_analysis_results.png` - Visualization charts
- Console output with summary statistics

## 🚀 Complete Workflow

### **Step 1: Select Evaluation Emails**
```bash
cd evaluation
python3 select_evaluation_emails.py
```
- Analyzes your `pairs_train.jsonl` file
- Selects 50 emails from 2020 onwards
- Saves to `evaluation_data/selected_evaluation_emails.json`

### **Step 2: Generate Dataset (Colab)**
```python
# Upload and run in Colab
!python generate_evaluation_dataset.py

# Download the dataset
files.download('evaluation_data/evaluation_dataset.json')
```

### **Step 3: Analyze Dataset (Local)**
```bash
# Place dataset in evaluation_data/ directory
python3 analyze_evaluation_dataset.py
```

## 📈 Interpreting Results

### **Success Indicators:**
- **LoRA AUC > GPT-4 AUC**: Your model better distinguishes your style
- **LoRA Cosine Similarity > GPT-4**: LoRA captures your style better
- **LoRA Delta < GPT-4 Delta**: LoRA is more similar to your writing
- **Positive improvement percentages**: LoRA outperforms GPT-4

### **Areas for Improvement:**
- **Low AUC scores**: Model needs better style differentiation
- **Low similarity scores**: Consider more training data
- **Negative improvements**: Review training approach

## 🛠️ Customization

### **Add More Metrics:**
Edit `AdvancedStyleAnalyzer` class to add:
- **Semantic Similarity**: BERT embeddings, semantic analysis
- **Emotional Tone**: Sentiment analysis, emotion detection
- **Domain-Specific Features**: Technical terminology, industry jargon
- **Temporal Patterns**: Writing style evolution over time

### **Modify Analysis:**
- **Feature Weighting**: Adjust importance of different metrics
- **Thresholds**: Change success criteria
- **Visualizations**: Add custom charts and graphs

## 🔍 Sample Analysis Output

```
📊 ANALYSIS SUMMARY
============================================================
Cosine Similarity:
  GPT-4 vs Original: 0.234 ± 0.089
  LoRA vs Original:  0.456 ± 0.123
  Improvement: +94.9%

Burrow's Delta:
  GPT-4 vs Original: 12.456 ± 3.234
  LoRA vs Original:  8.123 ± 2.567
  Improvement: +34.8%

Authorship Verification (AUC):
  GPT-4 vs Original: 0.623
  LoRA vs Original:  0.789
```

## 🐛 Troubleshooting

### **Common Issues:**
1. **OpenAI API Errors**: Check API key and rate limits
2. **Model Loading Failures**: Verify model paths in Colab
3. **Memory Issues**: Reduce batch size or use smaller models
4. **Analysis Errors**: Ensure all dependencies are installed

### **Performance Tips:**
- **Colab**: Use GPU runtime for faster LoRA generation
- **Local Analysis**: No GPU required, runs on CPU
- **Batch Processing**: Process emails in smaller batches if needed

## 📚 Academic Context

This evaluation approach implements state-of-the-art stylometric analysis:

- **Authorship Verification**: Machine learning-based style classification
- **Burrow's Delta**: Classical statistical authorship attribution
- **Character N-grams**: Sub-word level style analysis
- **Lexical Richness**: Advanced vocabulary diversity metrics
- **Syntactic Complexity**: Grammar and structure analysis

## 🤝 Support

For issues or questions:
1. Check console output for detailed error messages
2. Verify all dependencies are installed correctly
3. Ensure file paths and API keys are set correctly
4. Check GPU memory availability in Colab

---

**Ready to evaluate your LoRA model?** 

1. **Start with**: `python3 select_evaluation_emails.py`
2. **Generate dataset in Colab**: `generate_evaluation_dataset.py`
3. **Analyze locally**: `python3 analyze_evaluation_dataset.py`
4. **Review comprehensive results** in `evaluation_results/`
