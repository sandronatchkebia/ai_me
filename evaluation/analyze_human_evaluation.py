#!/usr/bin/env python3
"""
AI Me - Human Evaluation Experiment Analyzer

This script analyzes the results of human evaluation experiments where participants
were asked to identify whether responses were generated by:
1. Human (Aleks)
2. AI (ChatGPT/Base)
3. LoRA (Fine-tuned model)

The script calculates various metrics to understand how well the LoRA model
can "fool" humans into thinking it's human-generated.
"""

import os
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from typing import List, Dict, Tuple
from collections import defaultdict, Counter
import warnings
warnings.filterwarnings('ignore')

class HumanEvaluationAnalyzer:
    """Analyze human evaluation experiment results."""
    
    def __init__(self):
        self.results = {}
        self.aggregated_data = {}
        
    def load_experiment_data(self, experiment_dir: str = "evaluation_data/experiment") -> Dict:
        """Load all experiment CSV files."""
        print("üîç Loading human evaluation experiment data...")
        
        experiment_files = []
        for file in os.listdir(experiment_dir):
            if file.endswith('.csv') and 'raters_packet' in file:
                experiment_files.append(file)
        
        print(f"üìÅ Found {len(experiment_files)} experiment files")
        
        all_data = []
        for file in experiment_files:
            file_path = os.path.join(experiment_dir, file)
            df = pd.read_csv(file_path)
            
            # Extract experiment ID from filename
            exp_id = file.split(' - ')[0].replace('AleksAITest', '')
            
            # Add experiment metadata
            df['experiment_id'] = exp_id
            df['rater_id'] = file.split('_')[-1].replace('.csv', '')
            
            all_data.append(df)
            
            print(f"  ‚úÖ Loaded {file} ({len(df)} responses)")
        
        # Combine all data
        combined_df = pd.concat(all_data, ignore_index=True)
        print(f"üìä Total responses: {len(combined_df)}")
        
        return combined_df
    
    def calculate_deception_metrics(self, df: pd.DataFrame) -> Dict:
        """Calculate key deception and identification metrics."""
        print("\nüîç Calculating deception metrics...")
        
        metrics = {}
        
        # 1. Overall Accuracy by Response Type
        accuracy_by_type = {}
        for response_type in ['Human', 'Base', 'LoRA']:
            type_data = df[df['key'] == response_type]
            if len(type_data) > 0:
                if response_type == 'Human':
                    # For Human responses, "Aleks" is correct
                    correct_guesses = (type_data['guess'] == 'Aleks').sum()
                else:
                    # For Base and LoRA responses, "AI" is correct
                    correct_guesses = (type_data['guess'] == 'AI').sum()
                
                total_guesses = len(type_data)
                accuracy = correct_guesses / total_guesses
                accuracy_by_type[response_type] = {
                    'accuracy': accuracy,
                    'correct': correct_guesses,
                    'total': total_guesses,
                    'deception_rate': 1 - accuracy  # How often it fooled people
                }
        
        metrics['accuracy_by_type'] = accuracy_by_type
        
        # 2. Confusion Matrix
        confusion_matrix = pd.crosstab(df['key'], df['guess'], normalize='index')
        metrics['confusion_matrix'] = confusion_matrix.to_dict()
        
        # 3. Deception Success Rate (how often each type was misidentified)
        deception_rates = {}
        for response_type in ['Human', 'Base', 'LoRA']:
            type_data = df[df['key'] == response_type]
            if len(type_data) > 0:
                if response_type == 'Human':
                    # For Human responses, "AI" guess is deception
                    misidentified = (type_data['guess'] == 'AI').sum()
                else:
                    # For Base and LoRA responses, "Aleks" guess is deception
                    misidentified = (type_data['guess'] == 'Aleks').sum()
                
                deception_rate = misidentified / len(type_data)
                deception_rates[response_type] = deception_rate
        
        metrics['deception_rates'] = deception_rates
        
        # 4. Human vs AI Detection Accuracy
        # Combine Base and LoRA as "AI" responses
        df['is_ai'] = df['key'].isin(['Base', 'LoRA'])
        df['guessed_ai'] = df['guess'].isin(['Base', 'LoRA'])
        
        ai_detection_accuracy = (df['is_ai'] == df['guessed_ai']).mean()
        metrics['ai_detection_accuracy'] = ai_detection_accuracy
        
        # 5. Confidence Analysis
        confidence_by_type = {}
        for response_type in ['Human', 'Base', 'LoRA']:
            type_data = df[df['key'] == response_type]
            if len(type_data) > 0:
                confidence_by_type[response_type] = {
                    'mean_confidence': type_data['confidence_1to5'].mean(),
                    'std_confidence': type_data['confidence_1to5'].std(),
                    'median_confidence': type_data['confidence_1to5'].median()
                }
        
        metrics['confidence_by_type'] = confidence_by_type
        
        # 6. Specific Misidentification Patterns
        misidentification_patterns = {}
        for response_type in ['Human', 'Base', 'LoRA']:
            type_data = df[df['key'] == response_type]
            if len(type_data) > 0:
                wrong_guesses = type_data[type_data['guess'] != response_type]
                if len(wrong_guesses) > 0:
                    misidentification_patterns[response_type] = wrong_guesses['guess'].value_counts().to_dict()
        
        metrics['misidentification_patterns'] = misidentification_patterns
        
        # 7. LoRA vs Base Performance Comparison
        lora_data = df[df['key'] == 'LoRA']
        base_data = df[df['key'] == 'Base']
        human_data = df[df['key'] == 'Human']
        
        if len(lora_data) > 0 and len(base_data) > 0 and len(human_data) > 0:
            # Deception = how often they were identified as "Aleks" (human)
            lora_deception = (lora_data['guess'] == 'Aleks').sum() / len(lora_data)
            base_deception = (base_data['guess'] == 'Aleks').sum() / len(base_data)
            human_human_rate = (human_data['guess'] == 'Aleks').sum() / len(human_data)
            
            # Calculate advanced metrics
            relative_human_likeness = (lora_deception - base_deception) / (human_human_rate - base_deception) if (human_human_rate - base_deception) > 0 else 0
            human_likeness_ratio = lora_deception / human_human_rate if human_human_rate > 0 else 0
            deception_efficiency = (lora_deception - base_deception) / (1 - base_deception) if (1 - base_deception) > 0 else 0
            
            metrics['lora_vs_base'] = {
                'lora_deception_rate': lora_deception,
                'base_deception_rate': base_deception,
                'human_human_rate': human_human_rate,
                'lora_advantage': lora_deception - base_deception,  # Positive means LoRA is more deceptive
                'relative_human_likeness': relative_human_likeness,  # How close to human performance
                'human_likeness_ratio': human_likeness_ratio,      # LoRA vs Human ratio
                'deception_efficiency': deception_efficiency       # Deception potential captured
            }
        
        return metrics
    
    def calculate_advanced_metrics(self, df: pd.DataFrame) -> Dict:
        """Calculate advanced psychological and statistical metrics."""
        print("üîç Calculating advanced metrics...")
        
        advanced_metrics = {}
        
        # 1. Response Length Analysis
        df['answer_length'] = df['answer'].str.len()
        length_by_type = df.groupby('key')['answer_length'].agg(['mean', 'std', 'count']).to_dict()
        advanced_metrics['length_analysis'] = length_by_type
        
        # 2. Confidence vs Accuracy Correlation
        df['correct_guess'] = (df['guess'] == df['key'])
        confidence_accuracy_corr = df['confidence_1to5'].corr(df['correct_guess'])
        advanced_metrics['confidence_accuracy_correlation'] = confidence_accuracy_corr
        
        # 3. Prompt Difficulty Analysis
        prompt_difficulty = df.groupby('prompt_id').agg({
            'correct_guess': 'mean',
            'confidence_1to5': 'mean'
        }).rename(columns={'correct_guess': 'difficulty_score'})
        advanced_metrics['prompt_difficulty'] = prompt_difficulty.to_dict()
        
        # 4. Rater Consistency Analysis
        rater_consistency = df.groupby('rater_id').agg({
            'correct_guess': 'mean',
            'confidence_1to5': 'mean'
        }).rename(columns={'correct_guess': 'accuracy', 'confidence_1to5': 'avg_confidence'})
        advanced_metrics['rater_consistency'] = rater_consistency.to_dict()
        
        # 5. Response Type Distribution
        response_distribution = df['key'].value_counts(normalize=True).to_dict()
        advanced_metrics['response_distribution'] = response_distribution
        
        return advanced_metrics
    
    def create_visualizations(self, metrics: Dict, output_dir: str = "evaluation_results"):
        """Create comprehensive visualizations."""
        print("üé® Creating visualizations...")
        
        os.makedirs(output_dir, exist_ok=True)
        plt.style.use('default')
        
        # Set up the plotting
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('Human Evaluation Experiment Results', fontsize=16, fontweight='bold')
        
        # 1. Deception Rates by Response Type
        if 'deception_rates' in metrics:
            types = list(metrics['deception_rates'].keys())
            rates = list(metrics['deception_rates'].values())
            
            bars = axes[0, 0].bar(types, rates, color=['#FF6B6B', '#4ECDC4', '#45B7D1'], alpha=0.8)
            axes[0, 0].set_title('Deception Success Rate\n(Higher = More Deceptive)')
            axes[0, 0].set_ylabel('Deception Rate')
            axes[0, 0].set_ylim(0, 1)
            
            # Add value labels
            for bar, rate in zip(bars, rates):
                axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, 
                               f'{rate:.2f}', ha='center', va='bottom')
        
        # 2. Accuracy by Response Type
        if 'accuracy_by_type' in metrics:
            types = list(metrics['accuracy_by_type'].keys())
            accuracies = [metrics['accuracy_by_type'][t]['accuracy'] for t in types]
            
            bars = axes[0, 1].bar(types, accuracies, color=['#FF6B6B', '#4ECDC4', '#45B7D1'], alpha=0.8)
            axes[0, 1].set_title('Identification Accuracy\n(Higher = Easier to Identify)')
            axes[0, 1].set_ylabel('Accuracy')
            axes[0, 1].set_ylim(0, 1)
            
            for bar, acc in zip(bars, accuracies):
                axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, 
                               f'{acc:.2f}', ha='center', va='bottom')
        
        # 3. Confidence Distribution
        if 'confidence_by_type' in metrics:
            types = list(metrics['confidence_by_type'].keys())
            mean_confidences = [metrics['confidence_by_type'][t]['mean_confidence'] for t in types]
            
            bars = axes[0, 2].bar(types, mean_confidences, color=['#FF6B6B', '#4ECDC4', '#45B7D1'], alpha=0.8)
            axes[0, 2].set_title('Average Confidence in Guesses')
            axes[0, 2].set_ylabel('Confidence (1-5)')
            axes[0, 2].set_ylim(0, 5)
            
            for bar, conf in zip(bars, mean_confidences):
                axes[0, 2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, 
                               f'{conf:.1f}', ha='center', va='bottom')
        
        # 4. Confusion Matrix Heatmap
        if 'confusion_matrix' in metrics:
            # Convert back to DataFrame for plotting
            conf_matrix = pd.DataFrame(metrics['confusion_matrix'])
            sns.heatmap(conf_matrix, annot=True, fmt='.2f', cmap='Blues', ax=axes[1, 0])
            axes[1, 0].set_title('Confusion Matrix\n(Normalized by True Label)')
            axes[1, 0].set_xlabel('Predicted Label')
            axes[1, 0].set_ylabel('True Label')
        
        # 5. LoRA vs Base Performance
        if 'lora_vs_base' in metrics:
            comparison_data = metrics['lora_vs_base']
            labels = ['LoRA', 'Base']
            deception_rates = [comparison_data['lora_deception_rate'], comparison_data['base_deception_rate']]
            
            bars = axes[1, 1].bar(labels, deception_rates, color=['#45B7D1', '#4ECDC4'], alpha=0.8)
            axes[1, 1].set_title('AI Deception Comparison\n(Higher = More Deceptive)')
            axes[1, 1].set_ylabel('Deception Rate')
            axes[1, 1].set_ylim(0, 1)
            
            for bar, rate in zip(bars, deception_rates):
                axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, 
                               f'{rate:.2f}', ha='center', va='bottom')
        
        # 6. Response Length Distribution
        if 'length_analysis' in metrics:
            types = list(metrics['length_analysis'].keys())
            mean_lengths = [metrics['length_analysis'][t]['mean'] for t in types]
            
            bars = axes[1, 2].bar(types, mean_lengths, color=['#FF6B6B', '#4ECDC4', '#45B7D1'], alpha=0.8)
            axes[1, 2].set_title('Average Response Length')
            axes[1, 2].set_ylabel('Characters')
            
            for bar, length in zip(bars, mean_lengths):
                axes[1, 2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5, 
                               f'{length:.0f}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'human_evaluation_results.png'), dpi=300, bbox_inches='tight')
        print(f"‚úÖ Visualizations saved to {output_dir}/human_evaluation_results.png")
        
        return fig
    
    def generate_report(self, metrics: Dict, advanced_metrics: Dict) -> str:
        """Generate a comprehensive text report."""
        print("üìù Generating comprehensive report...")
        
        report = []
        report.append("üéØ HUMAN EVALUATION EXPERIMENT RESULTS")
        report.append("=" * 60)
        report.append("")
        
        # Key Findings
        report.append("üîç KEY FINDINGS:")
        report.append("")
        
        if 'deception_rates' in metrics:
            report.append("Deception Success Rates (Higher = More Deceptive):")
            for response_type, rate in metrics['deception_rates'].items():
                report.append(f"  {response_type}: {rate:.1%}")
            report.append("")
        
        if 'lora_vs_base' in metrics:
            lora_adv = metrics['lora_vs_base']['lora_advantage']
            relative_human = metrics['lora_vs_base']['relative_human_likeness']
            human_ratio = metrics['lora_vs_base']['human_likeness_ratio']
            deception_eff = metrics['lora_vs_base']['deception_efficiency']
            
            if lora_adv > 0:
                report.append(f"üéâ LoRA is {lora_adv:.1%} MORE deceptive than Base AI")
            else:
                report.append(f"‚ö†Ô∏è  LoRA is {abs(lora_adv):.1%} LESS deceptive than Base AI")
            
            report.append("")
            report.append("üî¨ ADVANCED METRICS:")
            report.append(f"  Relative Human-Likeness: {relative_human:.1%}")
            report.append(f"  Human-Likeness Ratio: {human_ratio:.2f}x")
            report.append(f"  Deception Efficiency: {deception_eff:.1%}")
            report.append("")
        
        if 'ai_detection_accuracy' in metrics:
            ai_acc = metrics['ai_detection_accuracy']
            report.append(f"ü§ñ Overall AI Detection Accuracy: {ai_acc:.1%}")
            report.append("")
        
        # Detailed Analysis
        report.append("üìä DETAILED ANALYSIS:")
        report.append("")
        
        if 'accuracy_by_type' in metrics:
            report.append("Identification Accuracy by Response Type:")
            for response_type, data in metrics['accuracy_by_type'].items():
                report.append(f"  {response_type}: {data['accuracy']:.1%} ({data['correct']}/{data['total']})")
            report.append("")
        
        if 'confidence_by_type' in metrics:
            report.append("Average Confidence in Guesses:")
            for response_type, data in metrics['confidence_by_type'].items():
                report.append(f"  {response_type}: {data['mean_confidence']:.1f}/5")
            report.append("")
        
        # Advanced Insights
        if 'confidence_accuracy_correlation' in advanced_metrics:
            corr = advanced_metrics['confidence_accuracy_correlation']
            report.append(f"üß† Confidence vs Accuracy Correlation: {corr:.3f}")
            if abs(corr) > 0.3:
                report.append("  ‚Üí Strong correlation between confidence and accuracy")
            elif abs(corr) > 0.1:
                report.append("  ‚Üí Moderate correlation between confidence and accuracy")
            else:
                report.append("  ‚Üí Weak correlation between confidence and accuracy")
            report.append("")
        
        # Recommendations
        report.append("üí° RECOMMENDATIONS:")
        report.append("")
        
        if 'lora_vs_base' in metrics:
            lora_adv = metrics['lora_vs_base']['lora_advantage']
            relative_human = metrics['lora_vs_base']['relative_human_likeness']
            human_ratio = metrics['lora_vs_base']['human_likeness_ratio']
            
            if relative_human > 1.0:
                report.append("üéâ LoRA is SUPERHUMAN - it's more human-like than actual humans!")
            elif relative_human > 0.8:
                report.append("‚úÖ LoRA is approaching human-level performance - excellent!")
            elif relative_human > 0.5:
                report.append("‚úÖ LoRA shows significant improvement over base AI")
            else:
                report.append("‚ö†Ô∏è  LoRA needs improvement to match human performance")
            
            if human_ratio > 1.0:
                report.append(f"üöÄ LoRA is {human_ratio:.1f}x more 'human' than actual humans!")
        
        report.append("")
        report.append("üéØ Next Steps:")
        report.append("  1. Collect more responses from remaining participants")
        report.append("  2. Analyze prompt-specific performance patterns")
        report.append("  3. Consider fine-tuning based on human feedback")
        
        return "\n".join(report)
    
    def analyze_experiment(self, experiment_dir: str = "evaluation_data/experiment") -> Dict:
        """Run complete analysis of the human evaluation experiment."""
        print("üöÄ Starting Human Evaluation Experiment Analysis...")
        print("=" * 60)
        
        # Load data
        df = self.load_experiment_data(experiment_dir)
        
        if df.empty:
            print("‚ùå No experiment data found!")
            return {}
        
        # Calculate metrics
        metrics = self.calculate_deception_metrics(df)
        advanced_metrics = self.calculate_advanced_metrics(df)
        
        # Create visualizations
        self.create_visualizations(metrics)
        
        # Generate report
        report = self.generate_report(metrics, advanced_metrics)
        
        # Save results
        results = {
            'metrics': metrics,
            'advanced_metrics': advanced_metrics,
            'report': report,
            'data_summary': {
                'total_responses': int(len(df)),
                'unique_prompts': int(df['prompt_id'].nunique()),
                'unique_raters': int(df['rater_id'].nunique()),
                'response_types': df['key'].value_counts().to_dict()
            }
        }
        
        # Save to file
        output_file = "evaluation_results/human_evaluation_analysis.json"
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        
        # Convert numpy types to Python native types for JSON serialization
        def convert_numpy_types(obj):
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, dict):
                return {key: convert_numpy_types(value) for key, value in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy_types(item) for item in obj]
            else:
                return obj
        
        results_serializable = convert_numpy_types(results)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results_serializable, f, indent=2, ensure_ascii=False)
        
        print(f"\n‚úÖ Analysis complete! Results saved to {output_file}")
        
        # Print report
        print("\n" + "=" * 60)
        print(report)
        
        return results

def main():
    """Main function to run the human evaluation analysis."""
    analyzer = HumanEvaluationAnalyzer()
    results = analyzer.analyze_experiment()
    
    if results:
        print(f"\nüéâ Human evaluation analysis completed successfully!")
        print(f"üìÅ Check evaluation_results/ for detailed results and visualizations")
    else:
        print(f"\n‚ùå Analysis failed!")

if __name__ == "__main__":
    main()
