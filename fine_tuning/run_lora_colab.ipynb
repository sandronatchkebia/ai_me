{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "# AI Me - LoRA Fine-tuning on Colab\n",
    "\n",
    "This notebook runs LoRA fine-tuning on Google Colab with GPU acceleration.\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. **Set your Hugging Face API key** in the cell below\n",
    "2. **Upload your dataset** (ai_me_chat.zip) when prompted\n",
    "3. **Run all cells** to start training\n",
    "\n",
    "## Requirements\n",
    "- Colab with GPU runtime (T4, V100, or A100)\n",
    "- Hugging Face account with access to Llama models\n",
    "- Dataset prepared by `prepare_dataset.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpu_check"
   },
   "source": [
    "## 1. Check GPU and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpu_check_cell"
   },
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi\n",
    "\n",
    "# Install stable versions\n",
    "!pip -q install -U \"transformers>=4.43.3\" \"accelerate>=0.32.0\" \"peft>=0.11.1\" \\\n",
    "  \"datasets>=2.20.0\" \"tokenizers>=0.19.1\" \"bitsandbytes>=0.43.2\" sentencepiece huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imports"
   },
   "source": [
    "## 2. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports_cell"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login, InferenceClient\n",
    "from google.colab import userdata, files\n",
    "from datasets import load_from_disk\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "# Check PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hf_login"
   },
   "source": [
    "## 3. Login to Hugging Face\n",
    "\n",
    "**Important**: Set your HF key in Colab's userdata first:\n",
    "1. Go to Colab menu: Runtime ‚Üí Manage secrets\n",
    "2. Add a new secret with key: `hf_key`\n",
    "3. Value: Your Hugging Face API token\n",
    "4. Click 'Add secret'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hf_login_cell"
   },
   "outputs": [],
   "source": [
    "# Login to Hugging Face\n",
    "try:\n",
    "    hf_key = userdata.get('hf_key')\n",
    "    if hf_key:\n",
    "        login(hf_key)\n",
    "        print(\"‚úÖ Successfully logged in to Hugging Face\")\n",
    "    else:\n",
    "        print(\"‚ùå HF key not found in userdata. Please set it in Runtime ‚Üí Manage secrets\")\n",
    "        print(\"Key name should be: hf_key\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error logging in: {e}\")\n",
    "    print(\"Please check your HF key in userdata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clone_repo"
   },
   "source": [
    "## 4. Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_repo_cell"
   },
   "outputs": [],
   "source": [
    "%cd /content\n",
    "!rm -rf ai_me\n",
    "!git clone https://github.com/sandronatchkebia/ai_me.git\n",
    "%cd ai_me\n",
    "print(f\"Repository cloned to: {!pwd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload_dataset"
   },
   "source": [
    "## 5. Upload and Extract Dataset\n",
    "\n",
    "**Upload your `ai_me_chat.zip` file when prompted below.**\n",
    "\n",
    "This should contain the dataset prepared by `prepare_dataset.py` in the `fine_tuning/dataset/ai_me_chat/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_dataset_cell"
   },
   "outputs": [],
   "source": [
    "# Upload dataset\n",
    "print(\"üìÅ Please upload your ai_me_chat.zip file:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Extract dataset\n",
    "print(\"üìÇ Extracting dataset...\")\n",
    "!unzip -o ai_me_chat.zip -d /content/ai_me/fine_tuning/dataset/ >/dev/null\n",
    "\n",
    "# Verify dataset\n",
    "print(\"üîç Verifying dataset...\")\n",
    "ds = load_from_disk(\"/content/ai_me/fine_tuning/dataset/ai_me_chat\")\n",
    "print(f\"Dataset loaded: {len(ds['train'])} training samples, {len(ds['validation'])} validation samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training"
   },
   "source": [
    "## 6. Start LoRA Training\n",
    "\n",
    "The training will use:\n",
    "- **Model**: Meta-Llama-3.1-8B-Instruct\n",
    "- **Method**: LoRA with 4-bit quantization\n",
    "- **Training**: 2 epochs, learning rate 1.5e-4\n",
    "- **Sequence length**: 2048 tokens\n",
    "- **Batch size**: 2 per device, 8 gradient accumulation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training_cell"
   },
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"üöÄ Starting LoRA training...\")\n",
    "print(\"This may take several hours depending on your GPU.\")\n",
    "\n",
    "!python fine_tuning/train_lora.py \\\n",
    "  --model_id \"meta-llama/Meta-Llama-3.1-8B-Instruct\" \\\n",
    "  --dataset_dir \"/content/ai_me/fine_tuning/dataset/ai_me_chat\" \\\n",
    "  --output_dir \"/content/ai_me/fine_tuning/out/ai_me_lora_llama3p1_8b\" \\\n",
    "  --epochs 2.0 --learning_rate 1.5e-4 --max_seq_len 2048 \\\n",
    "  --per_device_train_batch_size 2 --gradient_accumulation_steps 8 \\\n",
    "  --save_steps 400 --save_total_limit 2 --logging_steps 20 \\\n",
    "  --load_4bit --bf16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download_results"
   },
   "source": [
    "## 7. Download Results\n",
    "\n",
    "After training completes, download your fine-tuned LoRA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_results_cell"
   },
   "outputs": [],
   "source": [
    "# Create zip file of results\n",
    "print(\"üì¶ Creating results archive...\")\n",
    "!cd /content/ai_me/fine_tuning/out && zip -r ai_me_lora_latest.zip ai_me_lora_llama3p1_8b >/dev/null\n",
    "\n",
    "# Download results\n",
    "print(\"‚¨áÔ∏è Downloading results...\")\n",
    "files.download(\"/content/ai_me/fine_tuning/out/ai_me_lora_latest.zip\")\n",
    "\n",
    "print(\"‚úÖ Training complete! Your LoRA model has been downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "optional_drive"
   },
   "source": [
    "## Optional: Save to Google Drive\n",
    "\n",
    "If you want to save the results to Google Drive instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "drive_save_cell"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive (uncomment if needed)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# Save to Drive (uncomment if needed)\n",
    "# !zip -r /content/drive/MyDrive/ai_me_lora_latest.zip /content/ai_me/fine_tuning/out/ai_me_lora_llama3p1_8b\n",
    "# print(\"üíæ Results saved to Google Drive\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
