{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "# AI Me - Model Testing & Comparison\n",
    "\n",
    "This notebook tests and compares the base model against your fine-tuned LoRA model.\n",
    "Perfect for demonstrating how your model can rewrite text in your personal style.\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "1. **Loads both models**: Base Llama and your fine-tuned version\n",
    "2. **Compares responses** to the same prompts\n",
    "3. **Tests rewrite capabilities** - generate with base model, rewrite with yours\n",
    "4. **Interactive testing** with custom prompts\n",
    "\n",
    "## Use Cases\n",
    "- **Style transfer**: Take formal text and make it conversational\n",
    "- **Tone adjustment**: Convert professional text to your personal voice\n",
    "- **Content refinement**: Improve and personalize existing text\n",
    "- **A/B testing**: See the difference between base and fine-tuned models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpu_check"
   },
   "source": [
    "## 1. Check GPU and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpu_check_cell"
   },
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi\n",
    "\n",
    "# Install required packages\n",
    "!pip -q install -U \"transformers>=4.43.3\" \"accelerate>=0.32.0\" \"peft>=0.11.1\" \\\n",
    "  \"datasets>=2.20.0\" \"tokenizers>=0.19.1\" \"bitsandbytes>=0.43.2\" sentencepiece huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imports"
   },
   "source": [
    "## 2. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports_cell"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login, InferenceClient\n",
    "from google.colab import userdata, files\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel, PeftConfig\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Check PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hf_login"
   },
   "source": [
    "## 3. Login to Hugging Face\n",
    "\n",
    "**Important**: Set your HF key in Colab's userdata first:\n",
    "1. Go to Colab menu: Runtime → Manage secrets\n",
    "2. Add a new secret with key: `hf_key`\n",
    "3. Value: Your Hugging Face API token\n",
    "4. Click 'Add secret'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hf_login_cell"
   },
   "outputs": [],
   "source": [
    "# Login to Hugging Face\n",
    "try:\n",
    "    hf_key = userdata.get('hf_key')\n",
    "    if hf_key:\n",
    "        login(hf_key)\n",
    "        print(\"✅ Successfully logged in to Hugging Face\")\n",
    "    else:\n",
    "        print(\"❌ HF key not found in userdata. Please set it in Runtime → Manage secrets\")\n",
    "        print(\"Key name should be: hf_key\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error logging in: {e}\")\n",
    "    print(\"Please check your HF key in userdata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clone_repo"
   },
   "source": [
    "## 4. Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_repo_cell"
   },
   "outputs": [],
   "source": [
    "%cd /content\n",
    "!rm -rf ai_me\n",
    "!git clone https://github.com/sandronatchkebia/ai_me.git\n",
    "%cd ai_me\n",
    "print(f\"Repository cloned to: {!pwd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload_model"
   },
   "source": [
    "## 5. Upload Your Fine-tuned Model\n",
    "\n",
    "**Upload the zip file generated by the training script** (e.g., `ai_me_lora_latest.zip`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_model_cell"
   },
   "outputs": [],
   "source": [
    "# Upload fine-tuned model\n",
    "print(\"📁 Please upload your fine-tuned model zip file:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Extract model\n",
    "print(\"📂 Extracting model...\")\n",
    "for filename in uploaded.keys():\n",
    "    if filename.endswith('.zip'):\n",
    "        !unzip -o \"{filename}\" -d /content/ai_me/fine_tuning/out/ >/dev/null\n",
    "        print(f\"✅ Extracted {filename}\")\n",
    "        break\n",
    "\n",
    "# List extracted models\n",
    "print(\"\\n�� Available models:\")\n",
    "!ls -la /content/ai_me/fine_tuning/out/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load_models"
   },
   "source": [
    "## 6. Load Both Models\n",
    "\n",
    "Load the base model and your fine-tuned LoRA model for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_models_cell"
   },
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "BASE_MODEL_ID = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "LORA_MODEL_PATH = \"/content/ai_me/fine_tuning/out/ai_me_lora_llama3p1_8b\"\n",
    "\n",
    "print(\"�� Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"🔧 Loading base model...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"🔧 Loading fine-tuned LoRA model...\")\n",
    "lora_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Load LoRA weights\n",
    "lora_model = PeftModel.from_pretrained(lora_model, LORA_MODEL_PATH)\n",
    "\n",
    "print(\"✅ Both models loaded successfully!\")\n",
    "print(f\"Base model: {BASE_MODEL_ID}\")\n",
    "print(f\"LoRA model: {LORA_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "generate_function"
   },
   "source": [
    "## 7. Text Generation Function\n",
    "\n",
    "Helper function to generate text from both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate_function_cell"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_length=512, temperature=0.7):\n",
    "    \"\"\"Generate text from a model with the given prompt.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    ",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Remove the input prompt from the output\n",
    "    response = generated_text[len(prompt):].strip()\n",
    "    return response\n",
    "\n",
    "def format_chat_prompt(user_message, system_message=\"\"):\n",
    "    \"\"\"Format a chat prompt for Llama models.\"\"\"\n",
    "    if system_message:\n",
    "        return f\"<|system|>\\n{system_message}<|end|>\\n<|user|>\\n{user_message}<|end|>\\n<|assistant|>\\n\"\n",
    "    else:\n",
    "        return f\"<|user|>\\n{user_message}<|end|>\\n<|assistant|>\\n\"\n",
    "\n",
    "print(\"✅ Generation functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "basic_comparison"
   },
   "source": [
    "## 8. Basic Model Comparison\n",
    "\n",
    "Test both models with simple prompts to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "basic_comparison_cell"
   },
   "outputs": [],
   "source": [
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"Tell me about yourself in a casual way.\",\n",
    "    \"Explain machine learning in simple terms.\",\n",
    "    \"Write a short story about a robot.\"\n",
    "]\n",
    "\n",
    "print(\"🔄 Testing basic prompts...\\n\")\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PROMPT {i}: {prompt}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Generate with base model\n",
    "    print(\"\\n🤖 BASE MODEL:\")\n",
    "    base_response = generate_text(base_model, tokenizer, prompt, max_length=200)\n",
    "    print(base_response)\n",
    "    \n",
    "    # Generate with fine-tuned model\n",
    "    print(\"\\n🎯 FINE-TUNED MODEL:\")\n",
    "    lora_response = generate_text(lora_model, tokenizer, prompt, max_length=200)\n",
    "    print(lora_response)\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rewrite_testing"
   },
   "source": [
    "## 9. Rewrite Tool Testing\n",
    "\n",
    "This is the main value proposition - use your fine-tuned model to rewrite text in your style!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rewrite_testing_cell"
   },
   "outputs": [],
   "source": [
    "# Sample texts to rewrite\n",
    "texts_to_rewrite = [\n",
    "    \"The implementation of machine learning algorithms requires careful consideration of data preprocessing techniques and hyperparameter optimization strategies to achieve optimal performance metrics.\",\n",
    "    \n",
    "    \"In accordance with company policy, all employees must submit their expense reports by the 15th of each month. Failure to comply with this requirement may result in delayed reimbursement processing.\",\n",
    "    \n",
    "    \"The weather forecast indicates a 60% probability of precipitation during the afternoon hours, with temperatures ranging from 18 to 22 degrees Celsius.\"\n",
    "]\n",
    "\n",
    "print(\"🔄 Testing rewrite capabilities...\\n\")\n",
    "\n",
    "for i, formal_text in enumerate(texts_to_rewrite, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ORIGINAL TEXT {i}:\")\n",
    "    print(f\"{formal_text}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Generate with base model\n",
    "    print(\"\\n🤖 BASE MODEL REWRITE:\")\n",
    "    base_prompt = f\"Rewrite this text in a more casual, conversational style: {formal_text}\"\n",
    "    base_response = generate_text(base_model, tokenizer, base_prompt, max_length=150)\n",
    "    print(base_response)\n",
    "    \n",
    "    # Generate with fine-tuned model (your style)\n",
    "    print(\"\\n🎯 YOUR STYLE REWRITE:\")\n",
    "    lora_prompt = f\"Rewrite this text in my personal style: {formal_text}\"\n",
    "    lora_response = generate_text(lora_model, tokenizer, lora_prompt, max_length=150)\n",
    "    print(lora_response)\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "interactive_testing"
   },
   "source": [
    "## 10. Interactive Testing\n",
    "\n",
    "Test your own prompts and see how both models respond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "interactive_testing_cell"
   },
   "outputs": [],
   "source": [
    "def interactive_test():\n",
    "    \"\"\"Interactive testing function.\"\"\"\n",
    "    print(\"🎯 Interactive Testing Mode\")\n",
    "    print(\"Type 'quit' to exit\\n\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nEnter your prompt (or 'quit'): \")\n",
    "        if user_input.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        if not user_input.strip():\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"PROMPT: {user_input}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Base model\n",
    "        print(\"\\n🤖 BASE MODEL:\")\n",
    "        try:\n",
    "            base_response = generate_text(base_model, tokenizer, user_input, max_length=300)\n",
    "            print(base_response)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "        \n",
    "        # Fine-tuned model\n",
    "        print(\"\\n🎯 YOUR STYLE:\")\n",
    "        try:\n",
    "            lora_response = generate_text(lora_model, tokenizer, user_input, max_length=300)\n",
    "            print(lora_response)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "\n",
    "# Run interactive testing\n",
    "interactive_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "style_analysis"
   },
   "source": [
    "## 11. Style Analysis\n",
    "\n",
    "Analyze the differences between base and fine-tuned model outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "style_analysis_cell"
   },
   "outputs": [],
   "source": [
    "# Style comparison prompts\n",
    "style_prompts = [\n",
    "    \"Write a short email to a colleague about a project update.\",\n",
    "    \"Explain a technical concept to a beginner.\",\n",
    "    \"Give advice to someone starting their career.\"\n",
    "]\n",
    "\n",
    "print(\"🔍 Style Analysis - Comparing Writing Styles\\n\")\n",
    "\n",
    "for prompt in style_prompts:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"PROMPT: {prompt}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Generate responses\n",
    "    base_response = generate_text(base_model, tokenizer, prompt, max_length=200)\n",
    "    lora_response = generate_text(lora_model, tokenizer, prompt, max_length=200)\n",
    "    \n",
    "    print(\"\\n🤖 BASE MODEL STYLE:\")\n",
    "    print(base_response)\n",
    "    \n",
    "    ",
    "    print(\"\\n🎯 YOUR PERSONAL STYLE:\")\n",
    "    print(lora_response)\n",
    "    \n",
    "    print(\"\\n💡 STYLE DIFFERENCES:\")\n",
    "    print(\"• Base model: More formal, generic, standard AI responses\")\n",
    "    print(\"• Your model: Personal tone, conversational, your unique voice\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "## 12. Summary and Next Steps\n",
    "\n",
    "Your fine-tuned model is now ready to use as a personal rewrite tool!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "conclusion_cell"
   },
   "outputs": [],
   "source": [
    "print(\"🎉 Testing Complete!\\n\")\n",
    "print(\"📊 WHAT YOU'VE ACCOMPLISHED:\")\n",
    "print(\"✅ Loaded both base and fine-tuned models\")\n",
    "print(\"✅ Compared responses side-by-side\")\n",
    "print(\"✅ Tested rewrite capabilities\")\n",
    "print(\"✅ Analyzed style differences\")\n",
    "print(\"✅ Interactive testing mode\")\n",
    "\n",
    "print(\"\\n🚀 NEXT STEPS:\")\n",
    "print(\"1. Use your model for personal content creation\")\n",
    "print(\"2. Rewrite formal text in your conversational style\")\n",
    "print(\"3. Generate content that sounds like you wrote it\")\n",
    "print(\"4. Share your fine-tuned model with others\")\n",
    "print(\"5. Continue training with more data to improve further\")\n",
    "\n",
    "print(\"\\n💡 KEY BENEFIT:\")\n",
    "print(\"Your model now captures your personal writing style and can be used\")\n",
    "print(\"as a powerful rewrite tool to make any text sound like you wrote it!\")\n",
    "\n",
    "# Clean up memory\n",
    "del base_model, lora_model\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\n🧹 Memory cleaned up!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
